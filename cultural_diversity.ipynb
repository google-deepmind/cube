{
  "cells": [
    {
      "metadata": {
        "id": "DlqQ8dMgR10H"
      },
      "cell_type": "code",
      "source": [
        "#install necessary packages (refer to requirements.txt in root folder)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3WaSGXzVHAz"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import os\n",
        "from diffusers import DiffusionPipeline\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "import tqdm\n",
        "from vendi_score import vendi\n",
        "from PIL import Image\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "metadata": {
        "id": "sBY8KYIsVEFg"
      },
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "#OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\n",
        "#Gemini\n",
        "genai.configure(api_key = \"YOUR_GEMINI_API_KEY\")\n",
        "gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IWe4P89DjCaL"
      },
      "cell_type": "code",
      "source": [
        "def get_gemini_response(image_path, prompt):\n",
        "\n",
        "    image_pil = Image.open(image_path)\n",
        "    return gemini_model.generate_content([image_pil, prompt]).text\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM5CxxXiVHAz"
      },
      "outputs": [],
      "source": [
        "def get_gpt4_response(image_path, question):\n",
        "\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "      base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "      \"model\": \"gpt-4-turbo\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "            {\n",
        "              \"type\": \"text\",\n",
        "              \"text\": f\"{question}\"\n",
        "            },\n",
        "            {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"max_tokens\": 300\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "    return response.json()['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ufUvt7VHAz"
      },
      "outputs": [],
      "source": [
        "def prompt_stable_diffusion_xl(prompt,\n",
        "                               negative_prompt,\n",
        "                               base,\n",
        "                               refiner,\n",
        "                               use_refiner = False,\n",
        "                               n_steps= 40,\n",
        "                               HIGH_NOISE_FRAC = 0.8,\n",
        "                               seed=1):\n",
        "    # run both experts\n",
        "    image = base(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=n_steps,\n",
        "        denoising_end=HIGH_NOISE_FRAC,\n",
        "        output_type=\"latent\",\n",
        "        seed=seed\n",
        "    ).images\n",
        "    if use_refiner:\n",
        "        image = refiner(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=n_steps,\n",
        "            denoising_start=HIGH_NOISE_FRAC,\n",
        "            image=image,\n",
        "            seed=seed\n",
        "        ).images\n",
        "\n",
        "    else:\n",
        "        image = [F.to_pil_image(image[0].to(\"cpu\"))]\n",
        "\n",
        "    return image[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STS3spGFWn_a"
      },
      "source": [
        "Add parameters here.\n",
        "Use 'concept_space_samples' to provide examples of artifacts that are expected to be output from the T2I model. This will help increase geo-tagger accuracy.\n",
        "Use 'sample_prompt' for input prompt. The prompt needs to be under-specified.\n",
        "More details of the parameters in the paper: https://arxiv.org/abs/2407.06863\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qOxVMCjVHAz"
      },
      "outputs": [],
      "source": [
        "model_path =  \"stabilityai/stable-diffusion-xl-base-1.0\" #@param\n",
        "model_name = 'sdxl' #@param\n",
        "sample_prompt = \"Image of traditional clothing\" #@param\n",
        "concept_space_samples = \"sari, kimono, jeans, kurta\" #@param\n",
        "gpu_id = 0\n",
        "geo_tagger_vlm = 'gemini' # choose between {'gemini', 'gpt4'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsTXiq_uVHAz"
      },
      "source": [
        "Generate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e21aHwuSVHAz"
      },
      "outputs": [],
      "source": [
        "base = DiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
        "base.to(f\"cuda:{gpu_id}\")\n",
        "refiner = DiffusionPipeline.from_pretrained(model_path, text_encoder_2=base.text_encoder_2, vae=base.vae, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "refiner.to(f\"cuda:{gpu_id}\")\n",
        "num_images = 16\n",
        "save_folder = 'diversity_images'\n",
        "save_model_folder = os.path.join(save_folder, model_name)\n",
        "\n",
        "if not os.path.exists(save_model_folder):\n",
        "    os.makedirs(save_model_folder)\n",
        "\n",
        "for seed_ in tqdm.tqdm(range(num_images)):\n",
        "\n",
        "    img = prompt_stable_diffusion_xl(\n",
        "            prompt=sample_prompt,\n",
        "            base=base,\n",
        "            refiner=refiner,\n",
        "            use_refiner = True,\n",
        "            negative_prompt = \"multiple artifacts, blurry, painting, cartoon, artificial, nsfw, bad quality, bad anatomy, worst quality, low quality, low resolutions, extra fingers, blur, blurry, ugly, wrongs proportions, watermark, image artifacts, lowres, ugly, jpeg artifacts, deformed, noisy image\",\n",
        "            seed = seed_\n",
        "    )\n",
        "\n",
        "\n",
        "    image_name = f\"{seed_}.png\"\n",
        "    save_path = os.path.join(save_model_folder, image_name)\n",
        "    img.save(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y94LH8GHVHAz"
      },
      "source": [
        "Geo-tag images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8wQrDy6VHAz"
      },
      "outputs": [],
      "source": [
        "image_paths = []\n",
        "images = os.listdir(save_model_folder)\n",
        "for item in images:\n",
        "    item_path = os.path.join(save_model_folder, item)\n",
        "    if item[-3:] == 'png':\n",
        "        image_paths.append(item_path)\n",
        "\n",
        "\n",
        "question1 = f\"\"\"Given is an image generated for prompt: {sample_prompt}.\n",
        "                Which country is the artifact in image from?\n",
        "                Which continent does the country belong to?\n",
        "                What is the name of the cultural artifact? For example, cultural artifacts can be {concept_space_samples}\n",
        "                You need to tag the image to the closest country/continent and name the artifact.\n",
        "                Finally just output 3 terms \"continent, country and artifact name\" in a comma separated fashion and nothing else\".\n",
        "            \"\"\"\n",
        "\n",
        "all_annotations = []\n",
        "\n",
        "for image_path in tqdm.tqdm((image_paths)):\n",
        "\n",
        "    annotation = {}\n",
        "    annotation['image_name'] = image_path.split('/')[-1]\n",
        "    continent, country, artifact = '', '', ''\n",
        "    if geo_tagger_vlm == 'gemini':\n",
        "        [continent, country, artifact] = get_gemini_response(image_path, question1).strip().split(',')\n",
        "\n",
        "    elif geo_tagger_vlm == 'gpt4':\n",
        "\n",
        "        [continent, country, artifact] = get_gpt4_response(image_path, question1).strip().split(',')\n",
        "\n",
        "    annotation['label'] = {'continent': continent, 'country': country, 'artifact':artifact}\n",
        "\n",
        "    all_annotations.append(annotation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1WXf3a8VHAz"
      },
      "source": [
        "Calculate Cultural Diversity score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhHX8hYFVHAz"
      },
      "outputs": [],
      "source": [
        "def calculate_cultural_diversity(labels, similarity_function, _global=False, batch_size = 8):\n",
        "    \"\"\"Calculates normalized Vendi scores from annotation labels repeated over batches of 8 images.\n",
        "\n",
        "    Args:\n",
        "        labels: List of annotations loaded from a JSON file.\n",
        "        similarity_function: The function used to calculate similarity for Vendi score.\n",
        "        _global: Boolean to control if 'country' and 'continent' should be\n",
        "                 included in the 'samples' tuple. True indicates global prompts,\n",
        "                 False indicates within-culture prompts.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the mean and standard deviation of the normalized Vendi scores.\n",
        "    \"\"\"\n",
        "    if len(labels) \u003c 32:\n",
        "        labels = labels[:24]\n",
        "\n",
        "    chunks = [labels[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
        "    all_vendi = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if _global:\n",
        "            samples = [(item['continent'], item['country'], item['artifact']) for item in chunk]\n",
        "        else:\n",
        "            samples = [(item, None, None) for item in chunk]\n",
        "        if len(samples) == 0:\n",
        "            continue\n",
        "        normalized_vs = vendi.vendi_score(samples, similarity_function) / batch_size\n",
        "        all_vendi.append(normalized_vs)\n",
        "\n",
        "    return np.array(all_vendi).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FpnSBBLVHAz"
      },
      "outputs": [],
      "source": [
        "samples = [a['label'] for a in all_annotations]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOTPllx-VHAz"
      },
      "outputs": [],
      "source": [
        "similarity_function = lambda a, b: 1 * int(a[0]==b[0]) + 0 * int(a[1]==b[1]) + 0 * int(a[2]==b[2]) # hierarchical similarity function described in Section 5 of paper: https://arxiv.org/abs/2407.06863"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAhg-FBGVHAz"
      },
      "outputs": [],
      "source": [
        "calculate_cultural_diversity(samples, similarity_function, _global = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
